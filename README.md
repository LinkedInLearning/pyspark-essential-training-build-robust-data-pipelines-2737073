# PySpark Essential Training: Introduction to Building Data Pipelines
This is the repository for the LinkedIn Learning course `PySpark Essential Training: Introduction to Building Data Pipelines`. The full course is available from [LinkedIn Learning][lil-course-url].

![lil-thumbnail-url]

## Course Description

PySpark is a powerful library that brings Apache Spark’s distributed computing capabilities to Python, making it a key tool for processing large-scale data efficiently. In this course, data engineer and analyst Sam Bail provides a structured and hands-on introduction to PySpark, starting with an overview of Apache Spark, its architecture, and its ecosystem. Learn about Spark’s core concepts, such as the DataFrame API, transformations, lazy evaluations, and actions, before setting up a lab environment and working with a real dataset. Plus, gain insights into how PySpark fits into a broader data engineering ecosystem and best practices on running PySpark in a production environment.


## About This Repository

This repository contains the Jupyter Notebook file used in the PySpark Essential Training course. It contains all code shown in the course. You can install PySpark locally, clone this repository and run the Jupyter notebook on your own machine, or upload the notebook to Google Colab as shown in the course.

## Instructor

**Sam Bail**

Sam Bail is a data engineer with 10+ years in analytics, platforms, and technical leadership
                            

Check out my other courses on [LinkedIn Learning](https://www.linkedin.com/learning/instructors/sam-bail?u=104).


[0]: # (Replace these placeholder URLs with actual course URLs)

[lil-course-url]: https://www.linkedin.com/learning/pyspark-essential-training-introduction-to-building-data-pipelines
[lil-thumbnail-url]: https://media.licdn.com/dms/image/v2/D560DAQGsKtO4wEvWjw/learning-public-crop_675_1200/B56ZfrMgDbHQAY-/0/1751997613067?e=2147483647&v=beta&t=5401mig8RZRgtYV9iIGIkZRTFfk3zrwvzDtX7kGTPPo

